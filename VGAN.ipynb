{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.utils\n",
    "\n",
    "from new_data_loader import get_loader\n",
    "from make_gif import make_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator() :\n",
    "    discriminator_model = nn.Sequential(\n",
    "        nn.Conv3d (in_channels=3, out_channels=64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "        nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.BatchNorm3d(num_features=128, eps=1e-03),\n",
    "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "        nn.Conv3d(in_channels=128, out_channels=256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.BatchNorm3d(num_features=256, eps=1e-03),\n",
    "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "        nn.Conv3d(in_channels=256, out_channels=512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.BatchNorm3d(num_features=512, eps=1e-03),\n",
    "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "        nn.Conv3d(in_channels=512, out_channels=2, kernel_size=(2, 4, 4), stride=(1, 1, 1), padding=(0, 0, 0)),\n",
    "    )\n",
    "\n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator() :\n",
    "    net_video = nn.Sequential(\n",
    "        nn.ConvTranspose3d(in_channels = 100, out_channels = 512, kernel_size=(2,4,4)),\n",
    "        nn.BatchNorm3d(num_features=512),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.ConvTranspose3d(in_channels=512, out_channels=256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.BatchNorm3d(num_features=256),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.ConvTranspose3d(in_channels=256, out_channels=128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.BatchNorm3d(num_features=128),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.ConvTranspose3d(in_channels=128, out_channels=64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.BatchNorm3d(num_features=64),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "    gen_net = nn.Sequential(\n",
    "        net_video,\n",
    "        nn.ConvTranspose3d(in_channels=64, out_channels=3, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "\n",
    "    mask_net = nn.Sequential(\n",
    "        net_video,\n",
    "        nn.ConvTranspose3d(in_channels=64, out_channels=1, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    static_net = nn.Sequential(\n",
    "        nn.ConvTranspose2d(100, 512, 4, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(num_features=512),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(num_features=256),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(num_features=128),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(num_features=64),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "\n",
    "    return gen_net, mask_net, static_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_video(gen_net, mask_net, static_net, z) :\n",
    "    z_forward =  z.view(1, 100, 1, 1, 1)\n",
    "    z_backword = z.view(1, 100, 1, 1)\n",
    "\n",
    "    foreground = gen_net(z_forward)\n",
    "\n",
    "    mask = mask_net(z_forward).expand(1, 3, 32, 64, 64)\n",
    "\n",
    "    background = static_net(z_backword).view(1, 3, 1, 64, 64).expand(1, 3, 32, 64, 64)\n",
    "\n",
    "    video = foreground * mask + background * (1 - mask)\n",
    "\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m) :\n",
    "    name = type(m)\n",
    "\n",
    "    if name == nn.Conv3d or name == nn.ConvTranspose2d or name == nn.ConvTranspose3d :\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif name == nn.BatchNorm2d or name == nn.BatchNorm3d :\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#check GPU\n",
    "is_gpu = torch.cuda.is_available()\n",
    "print(is_gpu)\n",
    "\n",
    "if is_gpu :\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else :\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "D = discriminator()\n",
    "D = D.type(dtype)\n",
    "gen_net, mask_net, static_net = generator()\n",
    "\n",
    "gen_net = gen_net.type(dtype)\n",
    "mask_net = mask_net.type(dtype)\n",
    "static_net = static_net.type(dtype)\n",
    "\n",
    "D.apply(init_weights)\n",
    "gen_net.apply(init_weights)\n",
    "mask_net.apply(init_weights)\n",
    "static_net.apply(init_weights)\n",
    "\n",
    "real_labels = Variable(torch.ones(1, 2).type(dtype))\n",
    "fake_labels = Variable(torch.zeros(1, 2).type(dtype))\n",
    "criterion = nn.BCEWithLogitsLoss().type(dtype)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "g_optimizer = torch.optim.Adam(list(gen_net.parameters()) + list(mask_net.parameters()) + list(static_net.parameters()), lr=2e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1778], d_loss: 1.4894, g_loss: 3.7490\n",
      "Epoch [100/1778], d_loss: 2.0095, g_loss: 1.3315\n",
      "Epoch [200/1778], d_loss: 0.0652, g_loss: 9.1286\n",
      "Epoch [300/1778], d_loss: 0.4503, g_loss: 1.4827\n",
      "Epoch [400/1778], d_loss: 0.3078, g_loss: 0.3989\n",
      "Epoch [500/1778], d_loss: 2.1541, g_loss: 4.1623\n",
      "Epoch [600/1778], d_loss: 1.5747, g_loss: 5.9194\n",
      "Epoch [700/1778], d_loss: 2.0043, g_loss: 1.9075\n",
      "Epoch [800/1778], d_loss: 1.7176, g_loss: 1.7427\n",
      "Epoch [900/1778], d_loss: 0.4226, g_loss: 2.7318\n",
      "Epoch [1000/1778], d_loss: 0.2134, g_loss: 4.1878\n",
      "Epoch [1100/1778], d_loss: 2.9464, g_loss: 1.4353\n",
      "Epoch [1200/1778], d_loss: 2.3989, g_loss: 2.3121\n",
      "Epoch [1300/1778], d_loss: 8.5115, g_loss: 1.5465\n",
      "Epoch [1400/1778], d_loss: 0.0985, g_loss: 0.7140\n",
      "Epoch [1500/1778], d_loss: 2.3522, g_loss: 0.5888\n",
      "Epoch [1600/1778], d_loss: 0.4830, g_loss: 2.8065\n",
      "Epoch [1700/1778], d_loss: 0.2555, g_loss: 2.4308\n",
      "Epoch [0/1778], d_loss: 1.0218, g_loss: 4.2385\n",
      "Epoch [100/1778], d_loss: 2.0786, g_loss: 4.4158\n",
      "Epoch [200/1778], d_loss: 1.0211, g_loss: 1.2684\n",
      "Epoch [300/1778], d_loss: 0.2310, g_loss: 1.1859\n",
      "Epoch [400/1778], d_loss: 0.4042, g_loss: 0.2378\n",
      "Epoch [500/1778], d_loss: 0.5123, g_loss: 3.7873\n",
      "Epoch [600/1778], d_loss: 0.7611, g_loss: 1.7571\n",
      "Epoch [700/1778], d_loss: 0.3322, g_loss: 2.8946\n",
      "Epoch [800/1778], d_loss: 2.9315, g_loss: 1.2074\n",
      "Epoch [900/1778], d_loss: 1.8839, g_loss: 1.6188\n",
      "Epoch [1000/1778], d_loss: 0.8493, g_loss: 0.8785\n",
      "Epoch [1100/1778], d_loss: 2.6644, g_loss: 1.4361\n",
      "Epoch [1200/1778], d_loss: 0.3649, g_loss: 2.7641\n",
      "Epoch [1300/1778], d_loss: 0.5414, g_loss: 2.8437\n",
      "Epoch [1400/1778], d_loss: 0.2097, g_loss: 2.1690\n",
      "Epoch [1500/1778], d_loss: 1.1198, g_loss: 5.4072\n",
      "Epoch [1600/1778], d_loss: 0.0492, g_loss: 1.8098\n",
      "Epoch [1700/1778], d_loss: 1.1540, g_loss: 3.6494\n",
      "Epoch [0/1778], d_loss: 0.1973, g_loss: 2.1223\n",
      "Epoch [100/1778], d_loss: 0.0974, g_loss: 1.3089\n",
      "Epoch [200/1778], d_loss: 1.4393, g_loss: 0.9762\n",
      "Epoch [300/1778], d_loss: 0.2507, g_loss: 1.8028\n",
      "Epoch [400/1778], d_loss: 0.5768, g_loss: 0.6029\n",
      "Epoch [500/1778], d_loss: 0.3853, g_loss: 0.7764\n",
      "Epoch [600/1778], d_loss: 0.9845, g_loss: 1.4876\n",
      "Epoch [700/1778], d_loss: 0.6760, g_loss: 3.4714\n",
      "Epoch [800/1778], d_loss: 1.8541, g_loss: 0.6814\n",
      "Epoch [900/1778], d_loss: 1.1023, g_loss: 1.3865\n",
      "Epoch [1000/1778], d_loss: 0.6025, g_loss: 0.9445\n",
      "Epoch [1100/1778], d_loss: 2.7478, g_loss: 0.3063\n",
      "Epoch [1200/1778], d_loss: 0.3144, g_loss: 2.3330\n",
      "Epoch [1300/1778], d_loss: 0.0791, g_loss: 2.9569\n",
      "Epoch [1400/1778], d_loss: 1.0799, g_loss: 1.9753\n",
      "Epoch [1500/1778], d_loss: 1.3216, g_loss: 1.5142\n",
      "Epoch [1600/1778], d_loss: 3.1205, g_loss: 0.5117\n",
      "Epoch [1700/1778], d_loss: 0.0660, g_loss: 3.5733\n",
      "Epoch [0/1778], d_loss: 0.2908, g_loss: 1.9595\n",
      "Epoch [100/1778], d_loss: 0.0444, g_loss: 3.3455\n",
      "Epoch [200/1778], d_loss: 0.2614, g_loss: 3.6246\n",
      "Epoch [300/1778], d_loss: 0.8981, g_loss: 1.8433\n",
      "Epoch [400/1778], d_loss: 0.0324, g_loss: 3.6434\n",
      "Epoch [500/1778], d_loss: 0.0635, g_loss: 3.1855\n",
      "Epoch [600/1778], d_loss: 0.2138, g_loss: 2.4609\n",
      "Epoch [700/1778], d_loss: 0.1775, g_loss: 2.7677\n",
      "Epoch [800/1778], d_loss: 0.0485, g_loss: 3.9298\n",
      "Epoch [900/1778], d_loss: 0.0578, g_loss: 3.1737\n",
      "Epoch [1000/1778], d_loss: 0.1150, g_loss: 2.7912\n",
      "Epoch [1100/1778], d_loss: 0.1688, g_loss: 1.9209\n",
      "Epoch [1200/1778], d_loss: 0.0645, g_loss: 2.6759\n",
      "Epoch [1300/1778], d_loss: 0.1341, g_loss: 4.4786\n",
      "Epoch [1400/1778], d_loss: 0.7501, g_loss: 1.6009\n",
      "Epoch [1500/1778], d_loss: 0.1845, g_loss: 2.5783\n",
      "Epoch [1600/1778], d_loss: 4.6853, g_loss: 3.0044\n",
      "Epoch [1700/1778], d_loss: 0.0894, g_loss: 3.8140\n",
      "Epoch [0/1778], d_loss: 0.1661, g_loss: 2.6405\n",
      "Epoch [100/1778], d_loss: 0.0398, g_loss: 4.3309\n",
      "Epoch [200/1778], d_loss: 0.3843, g_loss: 1.5353\n",
      "Epoch [300/1778], d_loss: 1.7240, g_loss: 0.8509\n",
      "Epoch [400/1778], d_loss: 0.3969, g_loss: 3.3956\n",
      "Epoch [500/1778], d_loss: 0.0875, g_loss: 2.6693\n",
      "Epoch [600/1778], d_loss: 0.7682, g_loss: 1.9573\n",
      "Epoch [700/1778], d_loss: 0.1608, g_loss: 3.1553\n",
      "Epoch [800/1778], d_loss: 0.0254, g_loss: 3.9412\n",
      "Epoch [900/1778], d_loss: 0.1965, g_loss: 3.8273\n",
      "Epoch [1000/1778], d_loss: 0.0991, g_loss: 3.0802\n",
      "Epoch [1100/1778], d_loss: 0.1195, g_loss: 2.2478\n",
      "Epoch [1200/1778], d_loss: 0.0829, g_loss: 2.6825\n",
      "Epoch [1300/1778], d_loss: 0.0331, g_loss: 3.3418\n",
      "Epoch [1400/1778], d_loss: 0.3576, g_loss: 2.8635\n",
      "Epoch [1500/1778], d_loss: 0.1557, g_loss: 2.0275\n",
      "Epoch [1600/1778], d_loss: 2.9472, g_loss: 2.4384\n",
      "Epoch [1700/1778], d_loss: 0.0496, g_loss: 4.2180\n",
      "Epoch [0/1778], d_loss: 0.1023, g_loss: 2.2328\n",
      "Epoch [100/1778], d_loss: 0.1186, g_loss: 2.5584\n",
      "Epoch [200/1778], d_loss: 0.3051, g_loss: 1.6397\n",
      "Epoch [300/1778], d_loss: 2.6515, g_loss: 0.8637\n",
      "Epoch [400/1778], d_loss: 0.0314, g_loss: 3.4207\n",
      "Epoch [500/1778], d_loss: 0.0837, g_loss: 2.8565\n",
      "Epoch [600/1778], d_loss: 0.0209, g_loss: 4.2050\n",
      "Epoch [700/1778], d_loss: 0.1080, g_loss: 3.0093\n",
      "Epoch [800/1778], d_loss: 0.0340, g_loss: 3.2968\n",
      "Epoch [900/1778], d_loss: 0.0595, g_loss: 3.0498\n",
      "Epoch [1000/1778], d_loss: 0.1639, g_loss: 3.6384\n",
      "Epoch [1100/1778], d_loss: 0.2132, g_loss: 1.9549\n",
      "Epoch [1200/1778], d_loss: 0.0788, g_loss: 2.7228\n",
      "Epoch [1300/1778], d_loss: 0.1125, g_loss: 3.2744\n",
      "Epoch [1400/1778], d_loss: 0.4157, g_loss: 1.5896\n",
      "Epoch [1500/1778], d_loss: 0.3566, g_loss: 1.8231\n",
      "Epoch [1600/1778], d_loss: 4.0705, g_loss: 1.9645\n",
      "Epoch [1700/1778], d_loss: 0.1372, g_loss: 2.5283\n",
      "Epoch [0/1778], d_loss: 0.2282, g_loss: 2.7970\n",
      "Epoch [100/1778], d_loss: 0.0313, g_loss: 4.0841\n",
      "Epoch [200/1778], d_loss: 0.2932, g_loss: 1.7099\n",
      "Epoch [300/1778], d_loss: 3.0650, g_loss: 0.6004\n",
      "Epoch [400/1778], d_loss: 0.0452, g_loss: 3.2583\n",
      "Epoch [500/1778], d_loss: 0.0829, g_loss: 2.8588\n",
      "Epoch [600/1778], d_loss: 0.0621, g_loss: 3.2105\n",
      "Epoch [700/1778], d_loss: 0.0612, g_loss: 3.5744\n",
      "Epoch [800/1778], d_loss: 0.0552, g_loss: 3.2361\n",
      "Epoch [900/1778], d_loss: 0.1276, g_loss: 2.5031\n",
      "Epoch [1000/1778], d_loss: 0.0840, g_loss: 3.1288\n",
      "Epoch [1100/1778], d_loss: 0.0910, g_loss: 2.3328\n",
      "Epoch [1200/1778], d_loss: 0.3650, g_loss: 1.8794\n",
      "Epoch [1300/1778], d_loss: 1.0922, g_loss: 3.8761\n",
      "Epoch [1400/1778], d_loss: 0.0927, g_loss: 2.9997\n",
      "Epoch [1500/1778], d_loss: 0.1021, g_loss: 2.9382\n",
      "Epoch [1600/1778], d_loss: 2.1066, g_loss: 1.5412\n",
      "Epoch [1700/1778], d_loss: 0.1109, g_loss: 3.5610\n",
      "Epoch [0/1778], d_loss: 0.1701, g_loss: 2.7458\n",
      "Epoch [100/1778], d_loss: 0.0599, g_loss: 3.1500\n",
      "Epoch [200/1778], d_loss: 0.2132, g_loss: 2.0169\n",
      "Epoch [300/1778], d_loss: 1.7777, g_loss: 0.4832\n",
      "Epoch [400/1778], d_loss: 0.0116, g_loss: 4.6115\n",
      "Epoch [500/1778], d_loss: 0.0487, g_loss: 3.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-21:\n",
      "Process Process-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    return recv()\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    buf = self.recv_bytes()\n",
      "  File \"data_loader.py\", line 33, in __getitem__\n",
      "KeyboardInterrupt\n",
      "    image = self.transform(image)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/transforms.py\", line 199, in __call__\n",
      "    return img.resize(self.size, self.interpolation)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 1556, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-39f383f53f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# 1. Train Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/tensor.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, dest_type)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_loader = get_loader(data_path='./dataset', image_size=64, batch_size=1, num_workers=2)\n",
    "\n",
    "for epoch in range(1, 10) :\n",
    "\n",
    "    for iter, video in enumerate(data_loader) :\n",
    "        # 1. Train Discriminator\n",
    "        video_data = Variable(video).type(dtype)\n",
    "        \n",
    "\n",
    "        # 1-1. Real Video\n",
    "        outputs = D(video_data).view(1, 2)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "        # 1-2. Fake Video\n",
    "        z = Variable(torch.randn(100) * 0.01).type(dtype)\n",
    "        fake_videos = generate_video(gen_net, mask_net, static_net, z)\n",
    "        outputs = D(fake_videos).view(1, 2)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 2. Train Generator\n",
    "        z = Variable(torch.randn(100) * 0.01).type(dtype)\n",
    "        fake_videos = generate_video(gen_net, mask_net, static_net, z)\n",
    "        outputs = D(fake_videos).view(1, 2)\n",
    "\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "    \n",
    "        D.zero_grad()\n",
    "        gen_net.zero_grad() \n",
    "        mask_net.zero_grad()\n",
    "        static_net.zero_grad()\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if iter % 100 == 0 :\n",
    "            print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f' % (iter, len(data_loader), d_loss.data[0], g_loss.data[0]))\n",
    "\n",
    "    z = Variable(torch.randn(100) * 0.01).type(dtype)\n",
    "\n",
    "    for i in range(32) :\n",
    "        fake_video = torch.squeeze(generate_video(gen_net, mask_net, static_net, z))[:,i,:,:]\n",
    "        torchvision.utils.save_image(tensor=fake_video.data, filename=\"./test\" + str(i+1) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
