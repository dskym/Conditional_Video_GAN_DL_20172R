{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.utils\n",
    "\n",
    "from new_data_loader import get_loader\n",
    "from make_gif import make_gif\n",
    "\n",
    "from DiscrimiatorModule import Discriminator\n",
    "from GeneratorModule import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m) :\n",
    "    name = type(m)\n",
    "\n",
    "    if name == nn.Conv3d or name == nn.ConvTranspose2d or name == nn.ConvTranspose3d :\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif name == nn.BatchNorm2d or name == nn.BatchNorm3d :\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "pre_train = False\n",
    "\n",
    "batch_size = 64\n",
    "video_size = 64\n",
    "epoch_size = 1000\n",
    "        \n",
    "#check GPU\n",
    "is_gpu = torch.cuda.is_available()\n",
    "print(is_gpu)\n",
    "\n",
    "if is_gpu :\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else :\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "if pre_train :\n",
    "    D = torch.load('D2.ckpt').type(dtype)\n",
    "    G = torch.load('G2.ckpt').type(dtype)\n",
    "else :\n",
    "    D = Discriminator()\n",
    "    D = D.type(dtype)\n",
    "\n",
    "    G = Generator()\n",
    "    G = G.type(dtype)\n",
    "\n",
    "    D.apply(init_weights)\n",
    "    G.apply(init_weights)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().type(dtype)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader = get_loader(data_path='./dataset', image_size=video_size, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "for epoch in range(1, epoch_size + 1) :\n",
    "    for iter, (video, y) in enumerate(data_loader) :\n",
    "        local_batch_size = video.size()[0]\n",
    "        \n",
    "        real_labels = Variable(torch.ones(local_batch_size, 2).type(dtype))\n",
    "        fake_labels = Variable(torch.zeros(local_batch_size, 2).type(dtype))\n",
    "        \n",
    "        # 1. Train Discriminator\n",
    "        video_data = Variable(video).type(dtype)\n",
    "        y = Variable(y).type(dtype)\n",
    "        \n",
    "        y_data = torch.unsqueeze(torch.unsqueeze(torch.unsqueeze(y, -1), -1), -1)\n",
    "        y_data = y_data.expand(local_batch_size, 6, 32, 64, 64)\n",
    "       \n",
    "        \n",
    "        \n",
    "        # 1-1. Real Video\n",
    "        outputs = D(video_data, y_data).view(local_batch_size, 2)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "        \n",
    "        \n",
    "        # 1-2. Fake Video\n",
    "        z = Variable(torch.randn(local_batch_size, 100) * 0.01).type(dtype)\n",
    "        fake_videos = G(z, y)\n",
    "        outputs = D(fake_videos, y_data).view(local_batch_size, 2)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 2. Train Generator\n",
    "        z = Variable(torch.randn(local_batch_size, 100) * 0.01).type(dtype)\n",
    "        fake_videos = G(z, y)\n",
    "        outputs = D(fake_videos, y_data).view(local_batch_size, 2)\n",
    "\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "          \n",
    "        print('Epoch [%d/%d], Iter [%d/%d], d_loss: %.4f, g_loss: %.4f' % (epoch, epoch_size, iter, len(data_loader), d_loss.data[0], g_loss.data[0]))\n",
    "    \n",
    "    print('Model saving...')\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        torch.save(D, 'D_' + str(epoch) + '.ckpt')\n",
    "        torch.save(G, 'G_' + str(epoch) + '.ckpt')\n",
    "    else : \n",
    "        torch.save(D, 'D2.ckpt')\n",
    "        torch.save(G, 'G2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
